{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb04350",
   "metadata": {},
   "source": [
    "# Job 1 optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f8f02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.195:4041\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1757948759257)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f3475",
   "metadata": {},
   "source": [
    "### Schema definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1971a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import java.sql.Timestamp\n",
       "reviewSchema: org.apache.spark.sql.types.StructType = StructType(StructField(user_id,StringType,true),StructField(name,StringType,true),StructField(time,LongType,false),StructField(rating,DoubleType,true),StructField(text,StringType,true),StructField(pics,ArrayType(StringType,true),true),StructField(resp,StructType(StructField(time,LongType,false),StructField(text,StringType,true)),true),StructField(gmap_id,StringType,false))\n",
       "defined class Response\n",
       "defined class Review\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import java.sql.Timestamp\n",
    "\n",
    "val reviewSchema = StructType(\n",
    "  Seq(\n",
    "    StructField(\"user_id\",  StringType,            nullable = true),\n",
    "    StructField(\"name\",     StringType,            nullable = true),\n",
    "    StructField(\"time\",     LongType,              nullable = false),\n",
    "    StructField(\"rating\",   DoubleType,            nullable = true),\n",
    "    StructField(\"text\",     StringType,            nullable = true),\n",
    "    StructField(\"pics\",     ArrayType(StringType), nullable = true),\n",
    "    StructField(\"resp\",     StructType(\n",
    "      Seq(\n",
    "        StructField(\"time\", LongType,              nullable = false),\n",
    "        StructField(\"text\", StringType,            nullable = true)\n",
    "      )\n",
    "    ),                                             nullable = true),\n",
    "    StructField(\"gmap_id\",  StringType,            nullable = false),\n",
    "  )\n",
    ")\n",
    "\n",
    "case class Response(time: Timestamp, text: Option[String])\n",
    "\n",
    "case class Review(\n",
    "  user_id: Option[String],\n",
    "  name: Option[String],\n",
    "  time: Timestamp,\n",
    "  rating: Option[Double],\n",
    "  text: Option[String],\n",
    "  pics: Seq[String],\n",
    "  resp: Option[Response],\n",
    "  gmap_id: String\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf8ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metadataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(address,StringType,true),StructField(gmap_id,StringType,false),StructField(description,StringType,true),StructField(latitude,DoubleType,false),StructField(longitude,DoubleType,false),StructField(category,ArrayType(StringType,true),true),StructField(avg_rating,DoubleType,false),StructField(num_of_reviews,IntegerType,false),StructField(price,StringType,true),StructField(hours,ArrayType(ArrayType(StringType,true),true),true),StructField(MISC,MapType(StringType,ArrayType(StringType,true),true),true),StructField(state,StringType,true),StructField(relative_results,ArrayType(StringType,true),true),StructField(url,StringType,false))\n",
       "defined class Metadata\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metadataSchema = StructType(\n",
    "  Seq(\n",
    "    StructField(\"name\",             StringType,                                 nullable = true),\n",
    "    StructField(\"address\",          StringType,                                 nullable = true),\n",
    "    StructField(\"gmap_id\",          StringType,                                 nullable = false),\n",
    "    StructField(\"description\",      StringType,                                 nullable = true),\n",
    "    StructField(\"latitude\",         DoubleType,                                 nullable = false),\n",
    "    StructField(\"longitude\",        DoubleType,                                 nullable = false),\n",
    "    StructField(\"category\",         ArrayType(StringType),                      nullable = true),\n",
    "    StructField(\"avg_rating\",       DoubleType,                                 nullable = false),\n",
    "    StructField(\"num_of_reviews\",   IntegerType,                                nullable = false),\n",
    "    StructField(\"price\",            StringType,                                 nullable = true),\n",
    "    StructField(\"hours\",            ArrayType(ArrayType(StringType)),           nullable = true),\n",
    "    StructField(\"MISC\",             MapType(StringType, ArrayType(StringType)), nullable = true),\n",
    "    StructField(\"state\",            StringType,                                 nullable = true),\n",
    "    StructField(\"relative_results\", ArrayType(StringType),                      nullable = true),\n",
    "    StructField(\"url\",              StringType,                                 nullable = false),\n",
    "  )\n",
    ")\n",
    "\n",
    "case class Metadata(\n",
    "  name: Option[String],\n",
    "  address: Option[String],\n",
    "  gmap_id: String,\n",
    "  description: Option[String],\n",
    "  latitude: Double,\n",
    "  longitude: Double,\n",
    "  category: Seq[String],\n",
    "  avg_rating: Double,\n",
    "  num_of_reviews: Int,\n",
    "  price: Option[String],\n",
    "  hours: Seq[Seq[String]],\n",
    "  MISC: Map[String, Seq[String]],\n",
    "  state: Option[String],\n",
    "  relative_results: Seq[String],\n",
    "  url: String\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94003df",
   "metadata": {},
   "source": [
    "### Dataset load and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2d175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- pics: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- resp: struct (nullable = true)\n",
      " |    |-- time: timestamp (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |-- gmap_id: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- avg_rating: double (nullable = true)\n",
      " |-- num_of_reviews: integer (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- hours: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- MISC: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- relative_results: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: 6 feature warnings; for details, enable `:setting -feature' or `:replay -feature'\n",
       "import java.nio.file.Paths\n",
       "projectDir: String = /Users/lucatassi/Projects/big-data/big-data-project\n",
       "reviewsPath: String = /Users/lucatassi/Projects/big-data/big-data-project/dataset/sample-reviews.ndjson\n",
       "metadataPath: String = /Users/lucatassi/Projects/big-data/big-data-project/dataset/metadata.ndjson\n",
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1d6da03c\n",
       "reviewsDf: org.apache.spark.sql.Dataset[Review] = [user_id: string, name: string ... 6 more fields]\n",
       "metadataDf: org.apache.spark.sql.Dataset[Metadata] = [name: string, address: string ... 13 more fields]\n",
       "reviewsRdd: org.apache.spark.rdd.RDD[(Option[String], Option[String], java.sql.Timestamp, Option[Double], Option[String], Seq[String], Option[(java.sql.Timestamp, ...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.file.Paths\n",
    "\n",
    "val projectDir: String = Paths.get(System.getProperty(\"user.dir\")).getParent.getParent.getParent.toString\n",
    "val reviewsPath = s\"$projectDir/dataset/sample-reviews.ndjson\"\n",
    "val metadataPath = s\"$projectDir/dataset/metadata.ndjson\"\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"NDJSON Reader\")\n",
    "  .master(\"local[*]\") // Needed in local mode\n",
    "  .getOrCreate()\n",
    "\n",
    "val reviewsDf = spark.read\n",
    "  .schema(reviewSchema)\n",
    "  .json(reviewsPath)\n",
    "  .withColumn(\"pics\", when (col(\"pics\") isNull, array()) otherwise col(\"pics\"))\n",
    "  .withColumn(\"time\", from_unixtime(col(\"time\") / 1000).cast(\"timestamp\"))\n",
    "  .withColumn(\"resp\", \n",
    "    when (\n",
    "      col(\"resp\") isNotNull, \n",
    "      struct(\n",
    "        from_unixtime(col(\"resp.time\") / 1000).cast(\"timestamp\").alias(\"time\"),\n",
    "        col(\"resp.text\").cast(StringType).alias(\"text\")\n",
    "      )\n",
    "    ) otherwise lit(null)\n",
    "  )\n",
    "  .as[Review]\n",
    "\n",
    "val metadataDf = spark.read\n",
    "  .schema(metadataSchema)\n",
    "  .json(metadataPath)\n",
    "  .withColumn(\"category\", when (col(\"category\") isNull, array()) otherwise col(\"category\"))\n",
    "  .withColumn(\"hours\", when (col(\"hours\") isNull, array()) otherwise col(\"hours\"))\n",
    "  .withColumn(\"relative_results\", when (col(\"relative_results\") isNull, array()) otherwise col(\"relative_results\"))\n",
    "  .withColumn(\"MISC\",\n",
    "    when (\n",
    "      col(\"MISC\") isNotNull,\n",
    "      col(\"MISC\").cast(MapType(StringType, ArrayType(StringType)))\n",
    "    ) otherwise typedLit(Map.empty[String, Seq[String]])\n",
    "  )\n",
    "  .as[Metadata]\n",
    "\n",
    "reviewsDf.printSchema()\n",
    "metadataDf.printSchema()\n",
    "\n",
    "// Unforturnately, it seems that Spark does not support case classes in RDDs. It throws ArrayStoreException\n",
    "// when trying to collect the RDD... [see also [here](https://github.com/adtech-labs/spylon-kernel/issues/40)]\n",
    "val reviewsRdd = reviewsDf.rdd\n",
    "  .map(Review.unapply)\n",
    "  .map(_.get)\n",
    "  .map { case review @ (_, _, _, _, _, _, resp, _) => review.copy(_7 = resp.map(Response.unapply(_).get)) }\n",
    "val metaRdd = metadataDf.rdd.map(Metadata.unapply).map(_.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff867d7",
   "metadata": {},
   "source": [
    "### Optimized version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6a15e",
   "metadata": {},
   "source": [
    "The goal of this job is to understand, year by year, whether greater frequency in responding to reviews has an impact on the average rating received.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- For each year and business, the average rating, rate, and average response time are calculated;\n",
    "- Based on the rate and average response time, an additional attribute \"response strategy\" is calculated that categorizes the business in a particular year into four categories (\"Rapid and frequent,\" \"Slow but frequent,\" \"Occasional,\" or \"Rare or none\");\n",
    "- Aggregation based on the \"response strategy,\" year, and state to get the average rate and number of businesses within the category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fa176-261b-458a-a387-10a25e8f07ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Metadata**: (name, address, <ins>gmap_id</ins>, description, latitude, longitude, category, avg_rating, num_of_reviews, price, hours, misc, state, relative_results, url)\n",
    "\n",
    "**Review**: (user_id, name, time, rating, text, pics, responses, <ins>gmap_id</ins>)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3299cb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.concurrent.TimeUnit\n",
       "import org.apache.spark.sql.SaveMode\n",
       "import org.apache.spark.storage.StorageLevel\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.concurrent.TimeUnit\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.storage.StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c28cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "responseStrategy: (avgResponseRate: Double, avgResponseTime: Double)String\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    " * Determines the response strategy, namely a string-based indicator of how businesses respond to reviews based on the\n",
    " * provided `avgResponseRate` and `avgResponseTime`.\n",
    " */\n",
    "def responseStrategy(avgResponseRate: Double, avgResponseTime: Double): String =\n",
    "  (avgResponseRate, avgResponseTime) match {\n",
    "    case (rr, rt) if rr >= 0.5 && rt <= 4 * 24 => \"Rapid and frequent\"\n",
    "    case (rr, rt) if rr >= 0.5 => \"Slow but frequent\"\n",
    "    case (rr, _)  if rr >= 0.15 => \"Occasional\"\n",
    "    case _ => \"Rare or none\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7502276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "consideredStates: scala.collection.immutable.Map[String,String] = Map(New Mexico -> NM, Washington -> WA, Alabama -> AL, Mississippi -> MS, New Hampshire -> NH)\n",
       "StateNameRegex: scala.util.matching.Regex = \\b(New Mexico|Washington|Alabama|Mississippi|New Hampshire)\\b\n",
       "StateAbbrevRegex: scala.util.matching.Regex = \\b(NM|WA|AL|MS|NH)\\b\n",
       "toState: (address: Option[String])String\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/** This regex captures the state abbreviation between a comma and the ZIP code. */\n",
    "private val StateRegex = \"\"\",\\s*([A-Z]{2})\\s+\\d{5}\"\"\".r\n",
    "\n",
    "/** The map of states that are considered for the analysis. */\n",
    "val consideredStates = Map(\n",
    "  \"Alabama\" -> \"AL\",\n",
    "  \"Mississippi\" -> \"MS\",\n",
    "  \"New Hampshire\" -> \"NH\",\n",
    "  \"New Mexico\" -> \"NM\",\n",
    "  \"Washington\" -> \"WA\",\n",
    ")\n",
    "\n",
    "/** Regex to match state names in the address. */\n",
    "val StateNameRegex = s\"\"\"\\\\b(${consideredStates.keys.mkString(\"|\")})\\\\b\"\"\".r\n",
    "\n",
    "/** Regex to match state abbreviations in the address. */\n",
    "val StateAbbrevRegex = s\"\"\"\\\\b(${consideredStates.values.mkString(\"|\")})\\\\b\"\"\".r\n",
    "\n",
    "/**\n",
    " * Extracts the state from the given address.\n",
    " * @param address\n",
    " *   the optional address string\n",
    " * @return\n",
    " *   the state abbreviation or \"Unknown\" if no valid state is found\n",
    " * @see\n",
    " *   [[consideredStates]]\n",
    " */\n",
    "def toState(address: Option[String]): String = address\n",
    "  .flatMap { addr =>\n",
    "    StateRegex\n",
    "      .findFirstMatchIn(addr)\n",
    "      .map(_.group(1))\n",
    "      .orElse(StateNameRegex.findFirstMatchIn(addr).map(stateName => consideredStates(stateName.group(1))))\n",
    "      .orElse(StateAbbrevRegex.findFirstMatchIn(addr).map(_.group(1)))\n",
    "  }\n",
    "  .filter(consideredStates.values.toSeq.contains)\n",
    "  .getOrElse(\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f57e9",
   "metadata": {},
   "source": [
    "- Metadata: 292.5 MiB\n",
    "- Reviews: 9.8 GiB\n",
    "\n",
    "The \"basic\" (not optimized) job has 5 stages (c.f. `Job1 Basic`):\n",
    "\n",
    "| #Stage | Input | Output | Shuffle read | Shuffle write | Duration | Partitions |\n",
    "|--------|-------|--------|--------------|---------------|----------|------------|\n",
    "| 0: load metadata | 292.5 MiB | | | 8.9 MiB | 12 s | 3 |\n",
    "| 1: load reviews  | 9.8 GiB | | | 759.0 MiB | 1.6 min | 79 |\n",
    "| 2: join datasets | | | 767.8 MiB | 69.3 MiB | 53s | 79 |\n",
    "| 3: aggregate by key | | | 69.3 MiB | 2007.2 KiB | 3s | 79 | \n",
    "| 4: aggregate by key | | | 2007.2 KiB | 11.2 KiB | 2s | 1 |\n",
    "\n",
    "**Total time: 2.6 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb57c5f",
   "metadata": {},
   "source": [
    "<ins>Optimizations:</ins>\n",
    "\n",
    "- The most time-consuming stage, not considering the data loading, is the #2.\n",
    "This can be optimized by **pushing down the join of the reviews only when the reviews have already been aggregated**, hence **reducing the amount of data shuffled** across the network;\n",
    "\n",
    "    - Unfortunately, even if filtered, the metadata RDD is still too large, >42MiB, hence using broadcast join is not worth it.\n",
    "\n",
    "- To further filter out not used data, only the timestamp of the response, if any, is kept during the first reviews aggregation phase.\n",
    "\n",
    "- After the join the final `map` + `aggregateByKey` operations can be rewritten using `map` + `reduceByKey`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6b5207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "businessesStates: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[17] at map at <console>:35\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val businessesStates = metaRdd.map(b => b._3 -> toState(b._2)) // [(gmap_id, state)*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca8593f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewsInfo: org.apache.spark.rdd.RDD[((Int, String), (Double, Double, Double, String))] = MapPartitionsRDD[21] at mapValues at <console>:53\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsInfo = reviewsRdd\n",
    "  .filter(_._4.isDefined) // filter out reviews without a rating\n",
    "  .map { case (_, _, time, rating, _, _, resp, id) => \n",
    "    (time.toLocalDateTime.getYear, id) -> (time, rating.get, resp.map(_._1)) \n",
    "  }\n",
    "  .aggregateByKey((0.0, 0, 0L, 0))( // (ratings sum, #responses, sum of response time differences, #reviews)\n",
    "    (acc, v) => {\n",
    "      val (sumRatings, numResponses, sumResponseTimes, totalReviews) = acc\n",
    "      val (time, rating, response) = v\n",
    "      (\n",
    "        sumRatings + rating,\n",
    "        numResponses + (if (response.isDefined) 1 else 0),\n",
    "        sumResponseTimes + (if (response.isDefined) response.get.getTime - time.getTime else 0L),\n",
    "        totalReviews + 1\n",
    "      )\n",
    "    },\n",
    "    (r1, r2) => (r1._1 + r2._1, r1._2 + r2._2, r1._3 + r2._3, r1._4 + r2._4)\n",
    "  )\n",
    "  .mapValues { case (sumRatings, numResponses, sumResponseTimes, totalReviews) => \n",
    "    val avgResponseRate = numResponses.toDouble / totalReviews\n",
    "    val avgResponseTime = if (numResponses > 0) TimeUnit.MILLISECONDS.toHours(sumResponseTimes / numResponses) else Double.PositiveInfinity\n",
    "    (sumRatings / totalReviews, avgResponseRate, avgResponseTime, responseStrategy(avgResponseRate, avgResponseTime))\n",
    "  } // [((year, gmap_id), (avg_rating, response_rate, avg_response_time, response_strategy))*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c34ec5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome: org.apache.spark.rdd.RDD[((Int, String, String), Double)] = MapPartitionsRDD[28] at mapValues at <console>:41\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outcome = reviewsInfo\n",
    "  .map { case ((year, id), (avgRating, _, _, responseStrategy)) => id -> (year, responseStrategy, avgRating) }\n",
    "  .join(businessesStates) // [(gmap_id, ((year, response_strategy, avg_rating), state))*]\n",
    "  .map { case (_, ((year, responseStrategy, avgRating), state)) => (year, state, responseStrategy) -> (avgRating, 1) }\n",
    "  .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "  .mapValues { case (sumRatings, totalBusinesses) => sumRatings / totalBusinesses }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48fa89a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputDirPath: String = /Users/lucatassi/Projects/big-data/big-data-project/output\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputDirPath = s\"$projectDir/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "993906c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome.map { case ((year, state, responseStrategy), avgRating) => (year, state, responseStrategy, avgRating) }\n",
    "  .coalesce(1)\n",
    "  .toDF(\"year\", \"state\", \"response_strategy\", \"avg_rating\")\n",
    "  .write.format(\"csv\").option(\"header\", \"true\")\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(s\"file://$outputDirPath/job1-optimized-without-partitioning-output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87790f91",
   "metadata": {},
   "source": [
    "Results (c.f. `Job1 Optimized without partitioning`):\n",
    "\n",
    "| #Stage | Input | Output | Shuffle read | Shuffle write | Duration | Partitions |\n",
    "|--------|-------|--------|--------------|---------------|----------|------------|\n",
    "| 0: load metadata | 292.6 MiB | | | 8.9 MiB | 42s | 3 |\n",
    "| 1: load reviews  | 9.8 GiB | | | **66.0 MiB** | 1.5 min | 79 |\n",
    "| 2: aggregate by key | | | **66.0 MiB** | 56.2 MiB | **5s** | 79 |\n",
    "| 3: join | | | 65.1 MiB | 1999.5 KiB | 3s | 79 | \n",
    "| 4: reduce by key | | | 1999.5 KiB | 11.2 KiB | 2s | 1 |\n",
    "\n",
    "What we gained is the a drop in the times of the stage #2 thanks to the fact reviews data have been first aggregated and then shuffled: compared to the non-optimized job we reduced the shuffle read size from 767.8 MiB to 66.0 MiB ($\\delta$ = 701.8 MiB) and the duration from 53 to 5 seconds.\n",
    "\n",
    "Changing from `aggregateByKey` to `reduceByKey` does not produce observable performances increments.\n",
    "\n",
    "**Total time**: 1.7 minutes, **53% faster**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15e91b",
   "metadata": {},
   "source": [
    "<ins>Last optimization:</ins>\n",
    "Partitioning by `gmap_id` in $N=18$ partitions.\n",
    "$N = 18$ have been chosen empirically based on the cluster configuration and the tasks analysis from Spark UI since it provides a good balance between parallelism and overhead due to task scheduling.\n",
    "\n",
    "Moreover, since after the first reviews aggregation the data passes from 9.8GiB to 66.0 MiB a `coalesce` operation is applied to reduce the number of partitions and optimize the subsequent stages to avoid excessive task scheduling overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0426fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitions: Int = 18\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@12\n",
       "businessesStates: org.apache.spark.rdd.RDD[(String, String)] = ShuffledRDD[35] at partitionBy at <console>:40\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val partitions = 18\n",
    "val partitioner = new org.apache.spark.HashPartitioner(partitions)\n",
    "\n",
    "val businessesStates = metaRdd\n",
    "  .map(b => b._3 -> toState(b._2)) // [(gmap_id, state)*]\n",
    "  .partitionBy(partitioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e4a997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewsInfo: org.apache.spark.rdd.RDD[((Int, String), (Double, Double, Double, String))] = CoalescedRDD[40] at coalesce at <console>:66\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsInfo = reviewsRdd\n",
    "  .filter(_._4.isDefined) // filter out reviews without a rating\n",
    "  .map { case (_, _, time, rating, _, _, resp, id) =>\n",
    "    (time.toLocalDateTime.getYear, id) -> (time, rating.get, resp.map(_._1))\n",
    "  }\n",
    "  .aggregateByKey((0.0, 0, 0L, 0))( // (ratings sum, #responses, sum of response time differences, #reviews)\n",
    "    (acc, v) => {\n",
    "      val (sumRatings, numResponses, sumResponseTimes, totalReviews) = acc\n",
    "      val (time, rating, response) = v\n",
    "      (\n",
    "        sumRatings + rating,\n",
    "        numResponses + (if (response.isDefined) 1 else 0),\n",
    "        sumResponseTimes + (if (response.isDefined) response.get.getTime - time.getTime else 0L),\n",
    "        totalReviews + 1,\n",
    "      )\n",
    "    },\n",
    "    (r1, r2) => (r1._1 + r2._1, r1._2 + r2._2, r1._3 + r2._3, r1._4 + r2._4),\n",
    "  )\n",
    "  .mapValues { case (sumRatings, numResponses, sumResponseTimes, totalReviews) =>\n",
    "    val avgResponseRate = numResponses.toDouble / totalReviews\n",
    "    val avgResponseTime =\n",
    "      if (numResponses > 0) TimeUnit.MILLISECONDS.toHours(sumResponseTimes / numResponses)\n",
    "      else Double.PositiveInfinity\n",
    "    (\n",
    "      sumRatings / totalReviews,\n",
    "      avgResponseRate,\n",
    "      avgResponseTime,\n",
    "      responseStrategy(avgResponseRate, avgResponseTime),\n",
    "    )\n",
    "  } // [((year, gmap_id), (avg_rating, response_rate, avg_response_time, response_strategy))*]\n",
    "  .coalesce(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "822387af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome: org.apache.spark.rdd.RDD[((Int, String, String), Double)] = MapPartitionsRDD[48] at mapValues at <console>:43\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outcome = reviewsInfo\n",
    "  .map { case ((year, id), (avgRating, _, _, responseStrategy)) => id -> (year, responseStrategy, avgRating) }\n",
    "  .partitionBy(partitioner)\n",
    "  .join(businessesStates) // [(gmap_id, ((year, response_strategy, avg_rating), state))*]\n",
    "  .map { case (_, ((year, responseStrategy, avgRating), state)) => (year, state, responseStrategy) -> (avgRating, 1) }\n",
    "  .reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))\n",
    "  .mapValues { case (sumRatings, totalBusinesses) => sumRatings / totalBusinesses }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e22b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome\n",
    "  .map { case ((year, state, responseStrategy), avgRating) => (year, state, responseStrategy, avgRating) }\n",
    "  .coalesce(1)\n",
    "  .toDF(\"year\", \"state\", \"response_strategy\", \"avg_rating\")\n",
    "  .write.format(\"csv\").option(\"header\", \"true\")\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(s\"file://$outputDirPath/job1-optimized-output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f1d15",
   "metadata": {},
   "source": [
    "Finally (c.f. `Job1 Optimized`):\n",
    "\n",
    "| #Stage | Input | Output | Shuffle read | Shuffle write | Duration | Partitions |\n",
    "|--------|-------|--------|--------------|---------------|----------|------------|\n",
    "| 0: load metadata | 292.7 MiB | | | 8.7 MiB | 16s | 20 |\n",
    "| 1: load reviews  | 9.8 GiB | | | 66.0 MiB | 1.5 min | 79 |\n",
    "| 2: aggregate by key | | | 66.0 MiB | 54.6 MiB | 6s | 18 |\n",
    "| 3: partitionBy + join | | | 63.3 MiB | **185.6 KiB** | 3s | 18 | \n",
    "| 4: reduce by key | | | 185.6 KiB | 11.2 KiB | 1s | 1 |\n",
    "\n",
    "**Total time**: 1.7 min\n",
    "\n",
    "No significant changes in the overall execution time were observed.\n",
    "However, it is possible to see the overall amount of shuffled data during the 3rd stage has been reduced from 1.95 MiB (1999.5 KiB) to 185.6 KiB.\n",
    "Despite this reduction, the overall performance improvement is limited due to, very probably, the small size of data being processed.\n",
    "\n",
    "**The overall speedup achieved by the optimizations is around 53%.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
